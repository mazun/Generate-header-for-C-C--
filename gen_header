#!/usr/bin/env python
# coding: UTF-8

# config
database = "gen_header_database"
tmp_dir  = "gen_header_tmp"
tmp_filename_nondir = "gen_header_tmp.cpp"


import sys
import commands
import os
import shutil
from copy import deepcopy
from heapq import heappush, heappop

try:
    import json
except ImportError:
    import simplejson as json

tmp_filename = tmp_dir + os.sep + tmp_filename_nondir

class PriorityQueue(object):
    def __init__(self):
        self.queue = []
    def push(self, value):
        heappush(self.queue, value)
    def pop(self):
        return heappop(self.queue)
    def __len__(self):
        return len(self.queue)
    def __contains__(self, item):
        return item in self.queue

def unique (ls):
    ret = []
    if len(ls) == 0:
        return ret

    ls.sort()

    ret.append(ls[0])
    for i in range(len(ls) - 1):
        if ls[i + 1] != ls[i]:
            ret.append(ls[i + 1])

    return ret

def save_database (dictionary, flag):
    fp = open(database, flag)
    fp.write(json.dumps(dictionary))
    fp.close()

def load_database ():
    return json.load(open(database, "r"))

def get_tokens(filename):
    split_word = [" ", "\t", "<", ">", "{", "}", "[", "]", "(", ")", ";", "\r", "\n", ":", "#", ",", "+", "*", "/", "-", "|", "&", "^", "~", "."]

    tokens = []

    for line in open(filename):
        tokens.append(line)
    for sp in split_word:
        new_tokens = []
        for token in tokens:
            for t in token.split(sp):
                new_tokens.append(t)
        tokens = new_tokens
        if sp in tokens:
            tokens.remove(sp)

    if "" in tokens:
        tokens.remove("")

    return unique(tokens)

def gen_graph(tokens, dictionary):
    ret = []
    for i in range(len(dictionary)):
        ret.append([])

    tcnt = 0
    for t in tokens:
        dcnt = 0
        for d in dictionary:
            if (t in dictionary[d][1]):
                ret[dcnt].append(tcnt)
            dcnt = dcnt + 1
        tcnt = tcnt + 1

    return ret

def max_element(graph):
    ret = 0
    for g in graph:
        for gg in g:
            ret = max(ret, gg)
    return ret

def len_max_id(graph):
    mx = -1
    for i in range(len(graph)):
        g = graph[i]
        if mx < len(g):
            mx = len(g)
            ret = i
    return ret

# Solve problem by greedy
def solve_graph(graph, dictionary):
    n = len(graph)
    tmp = []
    for g in graph:
        for gg in g:
            tmp.append(gg)
    m = len(unique(tmp))
    ret = []
    cnt = 0

    while cnt != m:
        use = len_max_id(graph)
        ret.append(use)
        cnt = cnt + len(graph[use])

        for g in graph[use]:
            for gg in graph:
                if g in gg:
                    gg.remove(g)

    ans = []
    dcnt = 0
    for d in dictionary:
        if dcnt in ret:
            ans.append(d)
        dcnt = dcnt + 1

    return ans

def process_class(str):
    ret = deepcopy(str)

    pos = 0
    while (ret[pos:].find("class ") != -1) or (ret[pos:].find("struct ") != -1):
        tmp = ret[pos:].find("class ")
        if tmp == -1:
            tmp = ret[pos:].find("struct ")
        else:
            tmp2 = ret[pos:].find("struct ")
            if tmp2 != -1:
                tmp = min(tmp, tmp2)
        pos = pos + tmp

        while pos < len(ret) and (ret[pos] != ";" and ret[pos] != "{"):
            pos = pos + 1

        if pos >= len(ret):
            break

        if ret[pos] == "{":
            nest = 1
            pos1 = pos
            while nest > 0:
                pos = pos + 1
                if ret[pos] == "}":
                    nest = nest - 1
                elif ret[pos] == "{":
                    nest = nest + 1
            pos2 = pos

            ret = ret[0:pos1+1] + ret[pos2:]
            pos = pos1
    return ret

if __name__ == '__main__':

    argv = sys.argv
    argc = len(argv)

    if ("-h" in argv) or ("--help" in argv) or (argc == 1):
        ###
        # Print Usage
        ###

        print "Usage: " + argv[0] + " filename"
        print "       " + argv[0] + " --generate_database [option] headers"
        print "       " + argv[0] + " --generate_inline_database [option] headers"
        print "Options:"
        print "  " + "-a" + "\t" + "Add header to exisiting database"

    elif (argv[1] == "--generate_database") or (argv[1] == "--generate_inline_database"):
        ###
        # Generate database file
        ###

        files = argv[2:]
        inline = argv[1] == "--generate_inline_database"

        if("-a" in files):
            flag = "a"
            files.remove("-a")

            try:
                dictionary = load_database()
            except IOError:
                dictionary = {}
        else:
            flag = "w"
            dictionary = {}

        # Make tmp directory
        try:
            os.mkdir(tmp_dir)
        except OSError:
            ""

        for file in files:
            # Generate tmp files merely includes header file
            fp = open(tmp_filename, "w");
            if inline:
                fp.write("#include \"../" + file + "\"\n")
            else:
                fp.write("#include \"" + file + "\"\n")
            fp.close()

            # Compile tmp file with -E and -dN option
            so = commands.getstatusoutput("g++ -E -dN " + tmp_filename)
            status = so[0]
            output = so[1]

            if status != 0:
                print "An error occuerfed in processing (g++ -E -dN " + file + ")"
                shutil.rmtree(tmp_dir)
                exit()

            fp = open(tmp_filename, "w");
            fp.write(process_class(output))
            fp.close()

            # Analyze header file using ctags
            so = commands.getstatusoutput("cd " + tmp_dir + "; gtags; global -f " + tmp_filename_nondir)
            status = so[0]
            output = so[1]

            if status != 0:
                print "An error may occurrd in processing (gtags " + file + ")"
                shutil.rmtree(tmp_dir)
                exit()

            # symbols = [file]
            symbols = []
            for line in output.split("\n"):
                symbols.append(line.split(" ")[0])

            dictionary[file] = (inline, unique(symbols))

        shutil.rmtree(tmp_dir)
        save_database(dictionary, "w")

    else:
        ###
        # Normal mode
        ###

        dictionary = load_database()
        filename = argv[1]

        # For inline, we must do same thing 2 times.
        for iteration in range(2):
            # Get tokens
            tokens = get_tokens(filename)

            includes = solve_graph(gen_graph(tokens, dictionary), dictionary)

            as_str_inc = ""
            as_str_inl = ""

            contents = open(filename, "r").read()

            for inc in includes:
                if dictionary[inc][0]: # Inline?
                    s = open(inc, "r").read()
                    if contents.find(s) == -1:
                        as_str_inl = as_str_inl + s + "\n"
                else:
                    s = "#include <" + inc + ">"
                    if contents.find(s) == -1:
                        as_str_inc = as_str_inc + s + "\n"

            fp = open(filename, "w")
            fp.write(as_str_inc)
            fp.write(as_str_inl)
            fp.write(contents)
            fp.close()
